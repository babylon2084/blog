<!DOCTYPE html>
<html lang="en" data-color-mode="light">
<head>
  <meta charset="utf-8" />
  <title>Getting SAM 2 running on Windows without losing your mind · himal.blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta
    name="description"
    content="A step-by-step walkthrough of installing Meta’s Segment Anything 2.1 (SAM 2.1) on Windows with CUDA, plus a geospatial pipeline using segment-geospatial."
  />

  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link
    href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap"
    rel="stylesheet"
  />

  <script src="https://cdn.tailwindcss.com"></script>

  <style>
    /* ---------- COLOR MODES ---------- */

    :root[data-color-mode="light"] {
      --bg-color: #f9fafb;
      --fg-color: #0f172a;
      --muted-color: #4b5563;
      --subtle-color: #6b7280;
      --border-color: #e5e7eb;
      --border-strong: #cbd5f5;
      --card-bg: #ffffff;
      --chip-bg: #f3f4f6;
      --chip-border: #e5e7eb;
      --accent: #059669;
      --accent-soft: #d1fae5;
      --accent-strong: #10b981;
      --link-color: #2563eb;

      --code-bg: #020617;
      --code-border: #1f2937;

      --hero-gradient-from: #d1fae5;
      --hero-gradient-via: #22c55e;
      --hero-gradient-to: #0f172a;

      --meta-card-bg: #f9fafb;
    }

    :root[data-color-mode="dark"] {
      --bg-color: #020617;
      --fg-color: #e5e7eb;
      --muted-color: #9ca3af;
      --subtle-color: #64748b;
      --border-color: #1f2937;
      --border-strong: #334155;
      --card-bg: #020617;
      --chip-bg: #020617;
      --chip-border: #334155;
      --accent: #22c55e;
      --accent-soft: #064e3b;
      --accent-strong: #4ade80;
      --link-color: #60a5fa;

      --code-bg: #020617;
      --code-border: #1e293b;

      --hero-gradient-from: #064e3b;
      --hero-gradient-via: #059669;
      --hero-gradient-to: #020617;

      --meta-card-bg: #020617;
    }

    body {
      margin: 0;
      min-height: 100vh;
      background-color: var(--bg-color);
      color: var(--fg-color);
      font-family: 'Inter', system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
    }

    .page-shell {
      background:
        radial-gradient(900px 900px at 0% 0%, rgba(16,185,129,0.14), transparent 60%),
        radial-gradient(900px 900px at 100% 0%, rgba(56,189,248,0.12), transparent 60%),
        radial-gradient(900px 900px at 50% 100%, rgba(30,64,175,0.18), transparent 65%),
        var(--bg-color);
    }

    .card {
      background-color: var(--card-bg);
      border: 1px solid var(--border-color);
      border-radius: 1.25rem;
    }

    .chip-pill {
      background-color: var(--chip-bg);
      border-color: var(--chip-border);
      color: var(--subtle-color);
    }

/* minimal SAM2 hero  ------------------------------------- */

    .post-hero {
    border-radius: 1.5rem;
    padding: 1.75rem 1.75rem 1.9rem;
    border: 1px solid var(--border-color);
    background: var(--card-bg-strong, var(--card-bg));
    box-shadow: 0 16px 40px -28px rgba(15, 23, 42, 0.65);
    }

    /* lighten the hero a bit compared to other cards */
    :root[data-color-mode="dark"] .post-hero {
    background: linear-gradient(
        135deg,
        rgba(15, 118, 110, 0.18),
        rgba(15, 23, 42, 0.96)
    );
    }

    :root[data-color-mode="light"] .post-hero {
    background: #ffffff;
    box-shadow: 0 18px 45px -30px rgba(15, 23, 42, 0.16);
    }

    .post-hero-eyebrow {
    font-size: 0.68rem;
    letter-spacing: 0.22em;
    text-transform: uppercase;
    margin-bottom: 0.6rem;
    color: var(--subtle-color);
    }

    .post-hero-title {
    font-size: clamp(1.6rem, 2.3vw, 2.15rem);
    line-height: 1.2;
    font-weight: 600;
    margin-bottom: 0.75rem;
    color: var(--fg-color);
    }

    .post-hero-desc {
    font-size: 0.95rem;
    line-height: 1.6;
    color: var(--muted-color);
    max-width: 44rem;
    }

    .post-hero-meta {
    margin-top: 1.1rem;
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
    font-size: 0.75rem;
    }

    .post-hero-pill {
    padding: 0.35rem 0.7rem;
    border-radius: 999px;
    border: 1px solid var(--border-color);
    background: var(--chip-bg, rgba(15, 23, 42, 0.3));
    color: var(--fg-color);
    }

    /* lighter chips in light mode */
    :root[data-color-mode="light"] .post-hero-pill {
    background: #f7fafc;
    }


    .meta-card {
    border-radius: 1rem;
    padding: 1rem 1.15rem 1.1rem;
    background: var(--card-bg-soft);
    border: 1px solid var(--border-color-soft);
    box-shadow: 0 18px 45px -30px rgba(15, 23, 42, 0.65);
    font-size: 0.85rem;
    }

    .meta-card-header {
    display: flex;
    align-items: flex-start;
    justify-content: space-between;
    gap: 0.75rem;
    margin-bottom: 0.9rem;
    }

    .meta-eyebrow {
    font-size: 0.7rem;
    letter-spacing: 0.18em;
    text-transform: uppercase;
    font-weight: 600;
    color: var(--muted-color);
    }

    .meta-kicker {
    margin-top: 0.15rem;
    font-size: 0.8rem;
    color: var(--subtle-color);
    }

    .meta-pill {
    align-self: flex-start;
    padding: 0.25rem 0.7rem;
    border-radius: 999px;
    border: 1px solid var(--accent-soft-border);
    background: var(--accent-soft-bg);
    font-size: 0.68rem;
    letter-spacing: 0.16em;
    text-transform: uppercase;
    font-weight: 600;
    white-space: nowrap;
    }

    /* definition list layout */

    .meta-list {
    display: grid;
    row-gap: 0.55rem;
    margin-top: 0.2rem;
    }

    .meta-row {
    display: grid;
    grid-template-columns: minmax(0, 0.9fr) minmax(0, 1.6fr);
    column-gap: 0.75rem;
    align-items: baseline;
    }

    .meta-row dt {
    font-size: 0.78rem;
    text-transform: uppercase;
    letter-spacing: 0.14em;
    color: var(--muted-color);
    }

    .meta-row dd {
    margin: 0;
    font-size: 0.82rem;
    color: var(--body-color);
    }

    .meta-row a {
    text-decoration: underline;
    text-decoration-thickness: 1px;
    text-underline-offset: 2px;
    color: var(--link-color);
    }

    .meta-row a:hover {
    color: var(--link-color-strong);
    }

    /* footer line */

    .meta-footnote {
    margin-top: 0.9rem;
    padding-top: 0.6rem;
    border-top: 1px solid var(--border-color-soft);
    font-size: 0.78rem;
    color: var(--subtle-color);
    }

    /* ---------- CODE BLOCKS (MINIMAL) ---------- */

    .code-block {
    position: relative;
    margin-top: 0.85rem;
    margin-bottom: 1.5rem;
    padding: 0.85rem 1rem;
    border-radius: 0.85rem;
    background: var(--code-bg);
    border: 1px solid var(--code-border);
    font-family: 'JetBrains Mono', ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;
    font-size: 0.82rem;
    line-height: 1.5;
    overflow-x: auto;
    color: #e5e7eb;
    box-shadow: none;
    transition: background-color 150ms ease, border-color 150ms ease;
    }

    /* very subtle hover, no motion */
    .code-block:hover {
    background-color: rgba(15, 23, 42, 0.92);
    border-color: var(--border-strong);
    }

    .code-block code {
    white-space: pre;
    }


    /* ---------- COLOR MODE TOGGLE ---------- */
        :root[data-color-mode="dark"] {
    /* existing vars ... */
    --accent-soft-bg: rgba(16, 185, 129, 0.09);
    --accent-soft-border: rgba(45, 212, 191, 0.55);
    --border-color-soft: rgba(51, 65, 85, 0.9);
    }

    :root[data-color-mode="light"] {
    /* existing vars ... */
    --accent-soft-bg: rgba(16, 185, 129, 0.06);
    --accent-soft-border: rgba(16, 185, 129, 0.3);
    --border-color-soft: rgba(148, 163, 184, 0.6);
    }

    :root[data-color-mode="light"] .hidden_in_light_mode {
      display: none;
    }

    :root[data-color-mode="dark"] .hidden_in_dark_mode {
      display: none;
    }

    .color_mode_button {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      padding: 0.35rem;
      border-radius: 999px;
      border: 1px solid var(--border-color);
      background: transparent;
      cursor: pointer;
      transition:
        background-color 150ms ease,
        transform 150ms ease,
        border-color 150ms ease;
    }

    .color_mode_button svg {
      width: 1.1rem;
      height: 1.1rem;
      fill: none;
      stroke: var(--fg-color);
      stroke-linecap: round;
      stroke-linejoin: round;
      stroke-width: 1.6px;
    }

    .color_mode_button:hover {
      background-color: rgba(148,163,184,0.14);
      transform: translateY(-1px);
      border-color: var(--accent-soft);
    }

    .color_mode_button#enable_dark_mode:hover svg,
    .color_mode_button#enable_dark_mode:focus svg {
      outline: none;
      fill: rgba(56,189,248,0.18);
    }

    .color_mode_button#enable_light_mode:hover svg,
    .color_mode_button#enable_light_mode:focus svg {
      outline: none;
      fill: rgba(250,204,21,0.4);
    }

    /* ---------- SMALL UTILITIES ---------- */

    .prose-body p {
      margin-top: 0.75rem;
      margin-bottom: 0.75rem;
      color: var(--fg-color);
    }

    .prose-body h2 {
      font-size: 1.05rem;
      margin-top: 1.9rem;
      margin-bottom: 0.4rem;
      font-weight: 600;
    }

    .prose-body h3 {
      font-size: 0.98rem;
      margin-top: 1.3rem;
      margin-bottom: 0.35rem;
      font-weight: 600;
      color: var(--muted-color);
    }

    .prose-body ul {
      margin-top: 0.4rem;
      margin-bottom: 0.9rem;
      padding-left: 1.25rem;
      color: var(--fg-color);
    }

    .prose-body li {
      margin-top: 0.2rem;
      margin-bottom: 0.2rem;
    }

    .eyebrow {
      font-size: 0.7rem;
      letter-spacing: 0.24em;
      text-transform: uppercase;
      color: var(--subtle-color);
    }
  </style>
</head>

<body class="page-shell">
  <!-- SVG icons for sun/moon -->
  <svg style="display:none;" aria-hidden="true">
    <symbol viewBox="0 0 24 24" id="color_mode_control_icon_sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </symbol>
    <symbol viewBox="0 0 24 24" id="color_mode_control_icon_moon">
      <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
    </symbol>
  </svg>

  <!-- PAGE WRAP -->
  <div class="min-h-screen">
    <!-- HEADER -->
    <header class="sticky top-0 z-40 border-b border-[color:var(--border-color)] backdrop-blur bg-[color:transparent]">
      <div class="max-w-5xl mx-auto px-4 sm:px-6">
        <div class="h-14 flex items-center justify-between gap-4">
          <!-- Brand -->
          <a href="/index.html" class="font-mono text-xs tracking-[0.28em] uppercase text-[color:var(--subtle-color)]">
            HIMAL.BLOG
          </a>

          <!-- Center nav -->
          <nav class="hidden md:flex items-center gap-6 text-xs font-medium text-[color:var(--subtle-color)]">
            <a href="/index.html#posts" class="hover:text-[color:var(--fg-color)] transition">Home</a>
            <a href="https://himal.sh" target="_blank" rel="noreferrer" class="hover:text-[color:var(--fg-color)] transition">
              Portfolio
            </a>
          </nav>

          <!-- Right side: social + theme -->
          <div class="flex items-center gap-2">
            <!-- LinkedIn -->
            <a href="https://www.linkedin.com/in/gautamhimal"
               target="_blank" rel="noreferrer"
               class="p-1.5 rounded-full hover:bg-[rgba(148,163,184,0.16)] transition"
               aria-label="LinkedIn">
              <svg viewBox="0 0 24 24" class="w-4 h-4 text-[color:var(--fg-color)]">
                <path fill="currentColor" d="M4.98 3.5a2.5 2.5 0 1 0 0 5 2.5 2.5 0 0 0 0-5zM3.5 9h3v12h-3V9zm6 0h2.8v1.7h.04c.39-.74 1.34-1.7 2.76-1.7 2.95 0 3.9 1.94 3.9 4.46V21h-3v-6.2c0-1.48-.03-3.38-2.06-3.38-2.07 0-2.39 1.62-2.39 3.28V21h-3V9z"/>
              </svg>
            </a>
            <!-- GitHub -->
            <a href="https://github.com/himalgtm"
               target="_blank" rel="noreferrer"
               class="p-1.5 rounded-full hover:bg-[rgba(148,163,184,0.16)] transition"
               aria-label="GitHub">
              <svg viewBox="0 0 24 24" class="w-4 h-4 text-[color:var(--fg-color)]">
                <path fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"
                  d="M12 .5A11.5 11.5 0 0 0 .5 12c0 5.08 3.29 9.38 7.86 10.9.58.11.8-.25.8-.56v-2.1c-3.2.71-3.87-1.39-3.87-1.39-.53-1.36-1.3-1.72-1.3-1.72-1.06-.74.08-.73.08-.73 1.17.08 1.78 1.21 1.78 1.21 1.04 1.79 2.73 1.27 3.4.97.11-.76.41-1.27.74-1.56-2.55-.29-5.23-1.27-5.23-5.64 0-1.25.45-2.27 1.2-3.07-.12-.3-.52-1.53.11-3.18 0 0 .98-.31 3.21 1.17a11.1 11.1 0 0 1 5.85 0c2.23-1.48 3.21-1.17 3.21-1.17.63 1.65.23 2.88.11 3.18.75.8 1.2 1.82 1.2 3.07 0 4.39-2.69 5.34-5.25 5.63.42.36.8 1.06.8 2.15v3.2c0 .31.21.68.81.56A11.51 11.51 0 0 0 23.5 12 11.5 11.5 0 0 0 12 .5Z"/>
              </svg>
            </a>
            <!-- Theme toggles -->
            <button class="color_mode_button hidden_in_light_mode" id="enable_light_mode" aria-label="Switch to light mode">
              <svg><use href="#color_mode_control_icon_sun"></use></svg>
            </button>
            <button class="color_mode_button hidden_in_dark_mode" id="enable_dark_mode" aria-label="Switch to dark mode">
              <svg><use href="#color_mode_control_icon_moon"></use></svg>
            </button>
          </div>
        </div>
      </div>
    </header>

    <!-- MAIN -->
    <main class="max-w-5xl mx-auto px-4 sm:px-6 py-10">
      <!-- Breadcrumb + tiny nav -->
      <div class="flex flex-wrap items-center justify-between gap-3 mb-5 text-xs text-[color:var(--muted-color)]">
        <div class="flex items-center gap-2">
          <a href="/index.html#posts" class="hover:underline">← Back to all posts</a>
          <span aria-hidden="true" class="text-[color:var(--subtle-color)]">·</span>
          <span>GeoAI &amp; Tools</span>
        </div>
        <div class="flex flex-wrap gap-2">
          <a href="/posts/netflix-warner.html" class="hover:underline">Next: Netflix x Warner Bros rant →</a>
        </div>
      </div>

      <!-- Hero + meta -->
        <section class="mt-8 lg:mt-10 grid lg:grid-cols-[minmax(0,3fr)_minmax(0,1.35fr)] gap-6 items-start">
        <!-- Hero -->
        <article class="post-hero card">
            <p class="eyebrow mb-2">Computer Vision · GeoAI</p>
            <h1 class="text-[1.9rem] sm:text-[2.1rem] font-semibold tracking-tight">
            Getting SAM 2 running on Windows without losing your mind
            </h1>
            <p class="mt-3 text-[0.96rem] leading-relaxed">
            A step-by-step walkthrough of installing Meta’s Segment Anything Model 2.1 (SAM 2.1)
            on Windows, what’s actually going on under the hood, and how I plugged it into
            segment-geospatial to slice up a big aerial scene into ~300 segments with zero
            labeled data.
            </p>

            <div class="mt-4 flex flex-wrap gap-2 text-[0.78rem]">
            <span class="pill">SAM 2.1 · Hiera</span>
            <span class="pill">Windows · CUDA · PyTorch</span>
            <span class="pill">Dec 2025 · ~14 min read</span>
            </div>
        </article>

        <!-- Meta card -->
        <aside class="meta-card">
        <div class="meta-card-header">
            <div>
            <p class="meta-eyebrow">Reading guide</p>
            </div>
            <span class="meta-pill">SAM&nbsp;2.1 · Windows</span>
        </div>

        <dl class="meta-list">
            <div class="meta-row">
            <dt>Target setup</dt>
            <dd>NVIDIA GPU · Windows 11 · Conda · CUDA</dd>
            </div>
            <div class="meta-row">
            <dt>You’ll touch</dt>
            <dd>
                <a href="https://pytorch.org" target="_blank" rel="noreferrer">PyTorch</a>,
                <a href="https://github.com/facebookresearch/sam2" target="_blank" rel="noreferrer">SAM&nbsp;2 repo</a>,
                <a href="https://github.com/opengeos/segment-geospatial" target="_blank" rel="noreferrer">segment-geospatial</a>
            </dd>
            </div>
            <div class="meta-row">
            <dt>End goal</dt>
            <dd>GeoTIFF mask with ~300 segments from one aerial image</dd>
            </div>
        </dl>

        <p class="meta-footnote">
            Note: this post is my own write-up based on working with SAM&nbsp;2.1.
        </aside>


      <!-- ARTICLE BODY -->
      <article class="prose-body text-sm sm:text-[0.94rem] leading-relaxed">
        <p>
          For the last few weeks I’ve been living inside Meta’s Segment Anything Model 2.1 (SAM&nbsp;2.1)
          and a pile of aerial imagery.
        </p>
        <p>
          This post is my own breadcrumb trail: how I installed SAM&nbsp;2.1 on Windows, what’s actually
          going on under the hood, and how I plugged it into <code>segment-geospatial</code> to slice up a big
          aerial scene into ~300 segments with zero labeled data.
        </p>
        <p>
          If future-me forgets how any of this works, this is where I come back.
        </p>

        <!-- 1. Setting up -->
        <h2 id="setup">1. Setting Up SAM 2.1 on Windows</h2>
        <p>
          I’m running SAM&nbsp;2.1 inside a dedicated conda environment with Python 3.11 and GPU-enabled PyTorch.
        </p>

        <h3>1.1. Create a clean environment</h3>
        <p>I started with:</p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Terminal · Conda env</div>
          <pre class="code-block"><code>conda create -n sam2 python=3.11
conda activate sam2</code></pre>
        </div>
        <p>SAM 2.x wants Python ≥ 3.11, so I locked that in from the beginning.</p>

        <h3>1.2. Install PyTorch with CUDA</h3>
        <p>
          Inside that environment, I installed PyTorch + CUDA that matches my system. The exact command will
          depend on your GPU &amp; CUDA version, but the pattern looks like:
        </p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Terminal · PyTorch + CUDA</div>
          <pre class="code-block"><code># Example – replace with the command from PyTorch's website for your setup
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124</code></pre>
        </div>
        <p>I double-checked that CUDA was visible from Python:</p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Python</div>
          <pre class="code-block"><code>import torch
print(torch.cuda.is_available())  # True is what you want</code></pre>
        </div>
        <p>
          If this prints <code>False</code>, you’re not really using the GPU yet—time to fix your drivers / CUDA install.
        </p>

        <h3>1.3. Clone and install SAM 2</h3>
        <p>Then I pulled the official SAM&nbsp;2 repo and installed it in editable mode:</p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Terminal · SAM 2 repo</div>
          <pre class="code-block"><code>git clone https://github.com/facebookresearch/sam2.git
cd sam2
pip install -e ".[demo]"</code></pre>
        </div>
        <p>
          On Windows, that last line was a little flaky for me, so when it failed I just installed the
          important pieces manually:
        </p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Terminal · extra deps</div>
          <pre class="code-block"><code>pip install opencv-python matplotlib notebook</code></pre>
        </div>

        <h3>1.4. Download the SAM 2.1 checkpoints</h3>
        <p>
          Meta ships several model sizes (Hiera-B, Hiera-L, etc.). The repo has a helper script called
          <code>download_ckpts.sh</code>, but that’s a Bash script and Windows doesn’t love <code>.sh</code> by default.
        </p>
        <p>So I had two options:</p>
        <ul>
          <li>run the script in WSL / Git Bash, or</li>
          <li>
            manually download the <code>.pt</code> files from the links in the README / model card and drop them
            into a <code>checkpoints/</code> folder.
          </li>
        </ul>
        <p>Either way, I ended up with something like:</p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Project layout</div>
          <pre class="code-block"><code>sam2/
  checkpoints/
    sam2_hiera_large.pt
    sam2_hiera_base.pt
  configs/
    sam2_hiera_l.yaml
    sam2_hiera_b.yaml</code></pre>
        </div>

        <h3>1.5. Sanity check: can the model actually segment something?</h3>
        <p>Once the code and checkpoints were in place, I did a tiny inference test on a single image.</p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Python · quick smoke test</div>
          <pre class="code-block"><code>import cv2
import numpy as np
import torch

from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

device = "cuda" if torch.cuda.is_available() else "cpu"

checkpoint = "checkpoints/sam2_hiera_large.pt"
config = "configs/sam2_hiera_l.yaml"

# build model
sam2_model = build_sam2(config, checkpoint, device=device)
predictor = SAM2ImagePredictor(sam2_model)

# load an image
image = cv2.imread("test.jpg")
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# set the image for the predictor
predictor.set_image(image)

# point prompt at (100, 150)
point_coords = np.array([[100, 150]])
point_labels = np.array([1])   # 1 = foreground, 0 = background

masks, scores, logits = predictor.predict(
    point_coords=point_coords,
    point_labels=point_labels,
    multimask_output=True,
)

print("Got", len(masks), "candidate masks")</code></pre>
        </div>
        <p>
          If this runs without errors and prints a few masks, congrats: SAM&nbsp;2.1 is alive and talking to your GPU.
        </p>

        <h3>1.6. Little Windows gotchas</h3>
        <p>A couple of small potholes I hit and fixed:</p>
        <ul>
          <li>
            <strong>CUDA mismatch:</strong> PyTorch needs to match your installed CUDA toolkit. If things break during
            import, re-install PyTorch with the exact command from their website.
          </li>
          <li>
            <strong>CUDA_HOME:</strong> some builds want <code>CUDA_HOME</code> set. On Windows, I pointed it at my
            toolkit folder, e.g.:
          </li>
        </ul>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Command Prompt</div>
          <pre class="code-block"><code>set CUDA_HOME="C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4"</code></pre>
        </div>
        <p>
          Once all of that was stable, I moved from “cool demo” to “how does this thing actually work?”
        </p>

        <!-- 2. Mental model -->
        <h2 id="mental-model">2. How SAM 2.1 Thinks: A Quick Mental Model</h2>
        <p>
          Even if you’re only using SAM&nbsp;2.1 on still images at first, it helps to understand the video brain that
          sits behind it.
        </p>
        <p>Here’s the high-level breakdown in human terms:</p>

        <h3>2.1. Image encoder – Hiera</h3>
        <p>
          SAM&nbsp;2 uses a hierarchical Vision Transformer called Hiera as its image encoder.
        </p>
        <p>
          It processes each frame (or image) and produces a stack of multi-scale feature maps.
        </p>
        <p>
          For a single image, it feels very similar to SAM&nbsp;1’s ViT encoder: big image in → dense embedding out.
        </p>
        <p>
          Think of this as: “I’ve turned your pixels into a language I can reason about.”
        </p>

        <h3>2.2. Prompt encoder – your way of pointing</h3>
        <p>Just like SAM&nbsp;1, SAM&nbsp;2 listens to prompts:</p>
        <ul>
          <li>Points / boxes → turned into “sparse” embeddings with positional encodings.</li>
          <li>Masks as prompts → run through a small conv net to become a “dense” embedding.</li>
        </ul>
        <p>
          This is how you tell the model: “I care about this thing, not the whole scene.”
        </p>

        <h3>2.3. Memory bank – SAM’s long-term attention span</h3>
        <p>This is where SAM&nbsp;2 goes beyond SAM&nbsp;1.</p>
        <p>For videos, after you segment an object in a frame:</p>
        <ul>
          <li>SAM&nbsp;2 stores feature maps for that frame</li>
          <li>plus compact object pointers that summarize what that object looked like.</li>
        </ul>
        <p>When a new frame arrives, SAM&nbsp;2:</p>
        <ul>
          <li>runs the image encoder again,</li>
          <li>
            then uses memory attention to cross-attend the current features with memories from previous frames.
          </li>
        </ul>
        <p>
          Result: it can track the same object across time without you re-prompting every frame.
        </p>
        <p>
          For a single still image, there is no past, so the memory machinery is basically idle and SAM&nbsp;2 behaves
          like a very strong SAM&nbsp;1.
        </p>

        <h3>2.4. Mask decoder – turning understanding into pixels</h3>
        <p>The mask decoder:</p>
        <ul>
          <li>takes the image features (possibly enhanced with memory)</li>
          <li>and the prompt embeddings</li>
          <li>
            and runs a few rounds of transformer-style two-way attention to spit out segmentation masks.
          </li>
        </ul>
        <p>
          Just like SAM&nbsp;1, if a prompt is ambiguous, it can output multiple candidates. SAM&nbsp;2 adds an extra
          trick: a head that can say “the object is not in this frame” which matters a lot in video when things move
          off-screen.
        </p>

        <h3>2.5. Memory encoder – writing back to the brain</h3>
        <p>After predicting a mask for the current frame, SAM&nbsp;2 has a memory encoder that:</p>
        <ul>
          <li>compresses that mask + image features</li>
          <li>and writes them back into the memory bank (usually in some FIFO style).</li>
        </ul>
        <p>
          Over time, it builds a rolling memory of what the object has looked like in recent frames.
        </p>
        <p>
          SAM&nbsp;2.1 is basically an upgraded checkpoint of this architecture: same structure, just trained more and
          tuned better, especially for small / tricky / occluded things.
        </p>
        <p>
          Once I felt like I had a handle on that mental model, I moved to the fun part: geo stuff.
        </p>

        <!-- 3. segment-geospatial -->
        <h2 id="segment-geospatial">3. Bringing SAM into the Geospatial World with segment-geospatial</h2>
        <p>
          Next stop: <a href="https://github.com/opengeos/segment-geospatial" target="_blank" rel="noreferrer">segment-geospatial</a>
          (a.k.a. <code>samgeo</code>) – a Python package that wraps SAM for working with GeoTIFFs and other spatial imagery.
        </p>
        <p>The dream here is simple: “Give me a satellite or aerial image and let SAM carve it into meaningful regions without me drawing polygons for hours.”</p>

        <h3>3.1. Installing segment-geospatial</h3>
        <p>Inside the same <code>sam2</code> environment:</p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Terminal · samgeo</div>
          <pre class="code-block"><code>pip install -U segment-geospatial</code></pre>
        </div>
        <p>
          The author (Dr. Qiusheng Wu) has already wired in support for SAM&nbsp;2, usually exposed as <code>SamGeo2</code> in the API.
        </p>

        <h3>3.2. Test image</h3>
        <p>For my tests I used a high-resolution aerial image of a residential area:</p>
        <ul>
          <li>houses</li>
          <li>roads</li>
          <li>yards</li>
          <li>trees</li>
        </ul>
        <p>Basically the kind of scene you’d normally digitize by hand in GIS.</p>

        <!-- 4. Prompt attempt -->
        <h2>4. First Attempt: Prompt-based Segmentation (and a Facepalm)</h2>
        <p>
          My first instinct was: “Let’s click on a building and get just that building back.”
        </p>
        <p>
          I tried to use <code>SamGeo</code> directly in predictor mode and ran into:
        </p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Python traceback</div>
          <pre class="code-block"><code>AttributeError: 'SamGeo' object has no attribute 'predictor'</code></pre>
        </div>
        <p>
          Which was the library gently saying: “You’re using me in the wrong mode, my dude.”
        </p>
        <p>The important realization:</p>
        <ul>
          <li>For interactive prompts with SAM&nbsp;2, you want <code>SamGeo2</code></li>
          <li>and you want <code>automatic=False</code> so it acts like a predictor instead of running full automatic mask generation.</li>
        </ul>
        <p>Something like:</p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Python · predictor mode</div>
          <pre class="code-block"><code>from samgeo import SamGeo2

sam = SamGeo2(
    model_id="sam2-hiera-large",
    automatic=False,
)

sam.set_image("sample_aerial.tif")

mask = sam.predict(
    point_coords=[[x, y]],  # pixel coords
    point_labels=[1],       # 1 = foreground
)

sam.show_mask(mask)</code></pre>
        </div>
        <p>
          Once I re-wired my code that way, prompt-based prediction started making sense. But before I went too deep on
          that, I decided to see what full auto could do.
        </p>

        <!-- 5. Auto mask generation -->
        <h2>5. Letting SAM Go Wild: Automatic Mask Generation</h2>
        <p>
          To really stress test SAM&nbsp;2.1 on the aerial image, I switched to automatic segmentation.
        </p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Python · auto masks</div>
          <pre class="code-block"><code>from samgeo import SamGeo2

sam = SamGeo2(
    model_id="sam2-hiera-large",
    automatic=True,
)

sam.generate(
    source="sample_aerial.tif",
    output="output_mask.tif",
)</code></pre>
        </div>
        <p>This tells segment-geospatial:</p>
        <ul>
          <li>load the image (GeoTIFF or similar)</li>
          <li>run SAM’s Automatic Mask Generator</li>
          <li>save the result as a GeoTIFF mask, with each segment encoded as a unique integer.</li>
        </ul>

        <h3>5.1. Memory + bit depth issues</h3>
        <p>My test image was pretty large, so:</p>
        <ul>
          <li>the run was GPU + RAM heavy, but still finished</li>
          <li>the output contained ~294 segments (!)</li>
        </ul>
        <p>The first time I tried saving the mask, I hit:</p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Python error</div>
          <pre class="code-block"><code>OverflowError: Python int too large to convert to uint8</code></pre>
        </div>
        <p>
          Totally fair – 294 segment IDs don’t fit in an 8-bit range cleanly. The fix was to bump the output to a higher
          bit depth (e.g. <code>uint16</code>) so all IDs are preserved. Once I adjusted the save settings,
          <code>output_mask.tif</code> wrote cleanly.
        </p>

        <h3>5.2. What the result looked like</h3>
        <p>
          Visualizing that mask with a random colormap gave me a very colorful picture:
        </p>
        <ul>
          <li>each house = its own blob of color</li>
          <li>long continuous stretches of road = distinct segments</li>
          <li>trees and vegetation = separate patches</li>
        </ul>
        <p>
          The crazy part is: SAM&nbsp;2.1 was never trained specifically on satellite imagery, but still partitioned the
          scene into reasonable units just by visual distinctiveness.
        </p>
        <p>Zero labeled data. One pass. Nearly 300 regions.</p>

        <!-- 6. Achieved -->
        <h2>6. What I’ve Actually Achieved (So Far)</h2>
        <p>Here’s what I can do now, end-to-end:</p>
        <ul>
          <li>
            Spin up a Windows + CUDA + PyTorch environment and run SAM&nbsp;2.1 on GPU.
          </li>
          <li>
            Understand enough of the architecture (image encoder, prompts, memory, decoder) to reason about what it’s
            doing, not just treat it like magic.
          </li>
          <li>
            Use segment-geospatial to:
            <ul>
              <li>run automatic segmentation on an aerial image and write out a georeferenced mask</li>
              <li>get ~300 meaningful segments in one go, saved as a GeoTIFF</li>
            </ul>
          </li>
        </ul>
        <p>
          That’s already useful: I can convert that mask into polygons and feed them into QGIS / ArcGIS for measuring
          areas, filtering by size, or intersecting with other layers.
        </p>

        <!-- 7. Next steps -->
        <h2>7. Where I Want to Take This Next</h2>
        <p>
          I’m nowhere near done with this pipeline. The to-do list looks like this:
        </p>

        <h3>7.1. Filter and label the segments</h3>
        <p>
          SAM doesn’t know what the segments are, only that they’re visually distinct.
        </p>
        <p>Next step:</p>
        <ul>
          <li>identify which segments are buildings, roads, trees, water, etc.</li>
          <li>
            experiment with text prompts + CLIP-style filtering (segment-geospatial supports some of this)
          </li>
          <li>or bring in a simple classifier that works on crops of each segment.</li>
        </ul>

        <h3>7.2. Go back to interactive prompts</h3>
        <p>
          Now that I know how to correctly initialize <code>SamGeo2</code> in predictor mode, I want to:
        </p>
        <ul>
          <li>click once inside each building and instantly get precise building footprints</li>
          <li>use this to speed up manual mapping instead of replacing it entirely</li>
        </ul>
        <p>For that, the pattern will look like:</p>
        <div>
          <div class="code-label"><span class="code-label-dot"></span>Python · building masks</div>
          <pre class="code-block"><code>sam = SamGeo2(
    model_id="sam2-hiera-large",
    automatic=False,
)

sam.set_image("sample_aerial.tif")
mask = sam.predict(point_coords=[[x, y]], point_labels=[1])
sam.save_mask(mask, "building_mask.tif")</code></pre>
        </div>

        <h3>7.3. HQ-SAM and geospatial fine-tunes</h3>
        <p>
          The base SAM&nbsp;2.1 masks are good, but not perfect:
        </p>
        <ul>
          <li>edges can bleed across shadows</li>
          <li>roofs might be split into multiple small segments</li>
        </ul>
        <p>I want to try:</p>
        <ul>
          <li>HQ-SAM integrations in segment-geospatial for sharper boundaries</li>
          <li>dedicated geospatial fine-tunes like GeoSAM for road and infrastructure mapping</li>
        </ul>

        <h3>7.4. Post-processing and simplification</h3>
        <p>
          With ~294 segments, some are:
        </p>
        <ul>
          <li>too tiny</li>
          <li>or too fragmented to be useful.</li>
        </ul>
        <p>The plan:</p>
        <ul>
          <li>convert the raster mask → polygons</li>
          <li>dissolve tiny regions or adjacent pieces that clearly belong together</li>
          <li>drop noise segments with very small area</li>
        </ul>
        <p>Classic GIS cleanup.</p>

        <h3>7.5. Vectorization and full GIS workflows</h3>
        <p>Because the outputs are georeferenced, I can:</p>
        <ul>
          <li>export to Shapefile / GeoJSON</li>
          <li>pull them into QGIS / ArcGIS</li>
          <li>
            overlay with parcels, zoning, risk layers, etc.
          </li>
        </ul>
        <p>
          There’s even a QGIS plugin (<a href="https://github.com/opengeos/geoosam-qgis-plugin" target="_blank" rel="noreferrer">GeoOSAM</a>)
          using SAM&nbsp;2.1 directly, which tells me I’m not the only one walking this path.
        </p>

        <h3>7.6. Time series and change detection</h3>
        <p>This is the really fun idea:</p>
        <ul>
          <li>use SAM&nbsp;2.1’s video abilities on multi-temporal imagery</li>
          <li>treat each timestamp as a frame in a “video”</li>
          <li>track objects across time to see things like:</li>
          <ul>
            <li>deforestation</li>
            <li>new buildings</li>
            <li>changing water bodies</li>
          </ul>
        </ul>
        <p>
          Haven’t built this yet, but with the memory mechanism, SAM&nbsp;2.1 is basically begging to be used on time
          series.
        </p>

        <p>
          If you made it this far: thanks. This post is mostly here so I don’t forget how any of this works six months
          from now, but if it helps you get SAM&nbsp;2.1 running on your own machine, or convinces you to throw it at
          satellite images, then it did its job.
        </p>
      </article>
        </section>
    </main>

    <!-- FOOTER -->
    <footer class="border-t border-[color:var(--border-color)] py-6 text-xs sm:text-sm text-[color:var(--muted-color)]">
      <div class="max-w-5xl mx-auto px-4 sm:px-6 flex flex-col sm:flex-row items-center justify-between gap-3">
        <p>© <span id="yr"></span> Himal Gautam. Written from Denton, TX.</p>
        <div class="flex items-center gap-4">
          <a href="https://himal.sh" target="_blank" rel="noreferrer" class="hover:text-[color:var(--fg-color)]">Portfolio</a>
          <a href="mailto:himalgautam@my.unt.edu" class="hover:text-[color:var(--fg-color)]">Email</a>
          <a href="https://github.com/himalgtm" target="_blank" rel="noreferrer" class="hover:text-[color:var(--fg-color)]">GitHub</a>
        </div>
      </div>
    </footer>
  </div>

  <!-- COLOR MODE SCRIPT -->
  <script>
    // Respect system preference + remember user choice
    (function() {
      const matcher = window.matchMedia('(prefers-color-scheme: dark)');
      const stored = localStorage.getItem('data-color-mode');

      if (stored === 'dark' || (matcher.matches && !stored)) {
        document.documentElement.setAttribute('data-color-mode', 'dark');
      }

      matcher.addEventListener('change', (e) => {
        const storedMode = localStorage.getItem('data-color-mode');
        if (!storedMode) {
          document.documentElement.setAttribute('data-color-mode', e.matches ? 'dark' : 'light');
        }
      });
    })();

    document.querySelectorAll('.color_mode_button').forEach((button) => {
      button.addEventListener('click', (event) => {
        const targetElement = event.currentTarget;
        const elementId = targetElement.id;
        if (elementId === 'enable_light_mode') {
          document.documentElement.setAttribute('data-color-mode', 'light');
          localStorage.setItem('data-color-mode', 'light');
        } else if (elementId === 'enable_dark_mode') {
          document.documentElement.setAttribute('data-color-mode', 'dark');
          localStorage.setItem('data-color-mode', 'dark');
        }
      });
    });

    // Footer year
    document.getElementById('yr').textContent = new Date().getFullYear();
  </script>
</body>
</html>
